{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e1pykJRn0qKQ"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWnk6WoK0qKR",
        "outputId": "974683c6-cac2-4639-c34a-7a1f0ac75da0"
      },
      "outputs": [],
      "source": [
        "\n",
        "!python3 -m pip install tensorflow==2.13.0rc1\n",
        "!python3 -m pip install finta\n",
        "!python3 -m pip install -q --upgrade keras-nlp\n",
        "!python3 -m pip install numpy\n",
        "!python3 -m pip install pandas\n",
        "!python3 -m pip install matplotlib\n",
        "!python3 -m pip install plotly==5.3.1\n",
        "!python3 -m pip install opencv-python\n",
        "!python3 -m pip install -U kaleido\n",
        "!python3 -m pip install autokeras\n",
        "!python3 -m pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o8k9T-80qKT",
        "outputId": "559a3f13-d333-402d-eafe-b32c8f0e0d4e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from tensorflow.keras.models import load_model\n",
        "import keras_tuner\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from PIL import Image\n",
        "import io\n",
        "from finta import TA\n",
        "import os\n",
        "import autokeras as ak\n",
        "\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "print(tf.version.VERSION)\n",
        "print(ak.__version__)\n",
        "print(keras_tuner.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## create metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def win_rate(y_true, y_pred):\n",
        "\n",
        "    # convert y_pred to 0 or 1\n",
        "    y_pred = tf.where(y_pred < 0.5, 0, 1)\n",
        "\n",
        "    # convert the values to float32\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "    correct_count = tf.math.reduce_sum(tf.where(tf.math.logical_and(tf.math.equal(y_true, y_pred), tf.math.not_equal(y_pred, 0)), 1.0, 0.0))\n",
        "    total_count = tf.math.reduce_sum(tf.where(tf.math.not_equal(y_pred, 0), 1.0, 0.0))\n",
        "    win_rate = tf.math.divide_no_nan(correct_count, total_count)\n",
        "    return win_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if the path \"evaluation\" does not exist, create it\n",
        "if not os.path.exists(\"evaluation\"):\n",
        "    os.makedirs(\"evaluation\")\n",
        "\n",
        "def create_evaluation_folder(path):\n",
        "    # if \"evaluation/path\" does not exist, create it\n",
        "    if not os.path.exists(\"evaluation/\" + path):\n",
        "        os.makedirs(\"evaluation/\" + path)\n",
        "\n",
        "def record_stat(pair,stat):\n",
        "    # create the stat txt file if it does not exist\n",
        "    if not os.path.exists(\"evaluation/\" + pair + \"/stats.txt\"):\n",
        "        with open(\"evaluation/\" + pair + \"/stats.txt\", \"w\") as f:\n",
        "            f.write(\"\")\n",
        "    # add the stat as a new line in the txt file\n",
        "    with open(\"evaluation/\" + pair + \"/stats.txt\", \"a\") as f:\n",
        "        f.write(str(stat) + \"\\n\")\n",
        "\n",
        "def record_graph(pair,graph_name,graph):\n",
        "    # save the graph as a png\n",
        "    plt.savefig(\"evaluation/\" + pair + \"/\" + graph_name + \".png\", format='png')\n",
        "\n",
        "def predicted_wins_and_losses_amount(pair,overall_predictions):\n",
        "    # the amount of wins and losses predicted\n",
        "    win_count = np.count_nonzero(overall_predictions == 1)\n",
        "    loss_count = np.count_nonzero(overall_predictions == 0)\n",
        "    # record these stats\n",
        "    record_stat(pair,\"amount of wins predicted = \" + str(win_count))\n",
        "    record_stat(pair,\"amount of losses predicted = \" + str(loss_count))\n",
        "\n",
        "def accuracy(pair,overall_predictions,results):\n",
        "    correct_count = np.count_nonzero(overall_predictions == results)\n",
        "    total_count = len(results)\n",
        "    accuracy = correct_count / total_count\n",
        "    record_stat(pair,\"accuracy = \" + str(accuracy))\n",
        "\n",
        "def win_rate(pair,overall_predictions,results):\n",
        "    # calculate the win rate where overall_predictions == 1 and results == 1\n",
        "    wins = 0\n",
        "    amount_of_trades = 0\n",
        "    for i in range(len(overall_predictions)):\n",
        "        if overall_predictions[i] == 1:\n",
        "            amount_of_trades += 1\n",
        "            if results[i] == 1:\n",
        "                wins += 1\n",
        "    try:\n",
        "        win_rate = wins / amount_of_trades\n",
        "    except:\n",
        "        win_rate = 0\n",
        "    record_stat(pair,\"win rate = \" + str(win_rate))\n",
        "\n",
        "def accuracy_confidence_level(pair,predictions,overall_predictions,results):\n",
        "    # confidence level for the accuracy\n",
        "    win_confidence_tracker = []\n",
        "    loss_confidence_tracker = []\n",
        "    for i in range(len(predictions)):\n",
        "        confidence = ((0.5-predictions[i]) * 2) if overall_predictions[i] == 0 else ((predictions[i]-0.5) * 2)\n",
        "        if overall_predictions[i] == results[i]:\n",
        "            win_confidence_tracker.append(confidence)\n",
        "        else:\n",
        "            loss_confidence_tracker.append(confidence)\n",
        "    plt.clf()\n",
        "    plt.hist(win_confidence_tracker, bins=100, alpha=0.5, label='Correct')\n",
        "    plt.hist(loss_confidence_tracker, bins=100, alpha=0.5, label='Incorrect')\n",
        "    plt.legend(loc='upper right')\n",
        "    record_graph(pair,\"model_accuracy_confidence\",plt)\n",
        "\n",
        "def win_rate_confidence_level(pair,predictions,overall_predictions,results):\n",
        "    # confidence level for the win rate\n",
        "    win_confidence_tracker = []\n",
        "    loss_confidence_tracker = []\n",
        "    # for each prediction, if the prediction is correct, add the confidence to win_confidence_tracker, else add the confidence to loss_confidence_tracker\n",
        "    for i in range(len(predictions)):\n",
        "        if overall_predictions[i] != 0:\n",
        "            confidence = ((0.5-predictions[i]) * 2) if overall_predictions[i] == 0 else ((predictions[i]-0.5) * 2)\n",
        "            if overall_predictions[i] == results[i]:\n",
        "                win_confidence_tracker.append(confidence)\n",
        "            else:\n",
        "                loss_confidence_tracker.append(confidence)\n",
        "    plt.clf()\n",
        "    plt.hist(win_confidence_tracker, bins=100, alpha=0.5, label='Correct')\n",
        "    plt.hist(loss_confidence_tracker, bins=100, alpha=0.5, label='Incorrect')\n",
        "    plt.legend(loc='upper right')\n",
        "    record_graph(pair,\"model_winrate_confidence\",plt)\n",
        "\n",
        "def win_rate_confidence_level_with_RR(pair,predictions,overall_predictions,results):\n",
        "    # confidence level for the win rate including the risk to reward ratio\n",
        "    win_confidence_tracker = []\n",
        "    loss_confidence_tracker = []\n",
        "    # for each prediction, if the prediction is correct, add the confidence to win_confidence_tracker, else add the confidence to loss_confidence_tracker\n",
        "    for i in range(len(predictions)):\n",
        "        if overall_predictions[i] != 0:\n",
        "            confidence = ((0.5-predictions[i]) * 2) if overall_predictions[i] == 0 else ((predictions[i]-0.5) * 2)\n",
        "            if overall_predictions[i] == results[i]:\n",
        "                win_confidence_tracker.append(confidence)\n",
        "                win_confidence_tracker.append(confidence)\n",
        "            else:\n",
        "                loss_confidence_tracker.append(confidence)\n",
        "    plt.clf()\n",
        "    plt.hist(win_confidence_tracker, bins=100, alpha=0.5, label='Correct')\n",
        "    plt.hist(loss_confidence_tracker, bins=100, alpha=0.5, label='Incorrect')\n",
        "    plt.legend(loc='upper right')\n",
        "    record_graph(pair,\"model_winrate_confidence_RR\",plt)\n",
        "\n",
        "minimum_confidence = 0.9\n",
        "def win_rate_confidence_level_with_RR_over_90_confidence(pair,predictions,overall_predictions,results):\n",
        "    win_confidence_tracker = []\n",
        "    loss_confidence_tracker = []\n",
        "    wins = 0\n",
        "    amount_of_trades = 0\n",
        "    # for each prediction, if the prediction is correct, add the confidence to win_confidence_tracker, else add the confidence to loss_confidence_tracker\n",
        "    for i in range(len(predictions)):\n",
        "        if overall_predictions[i] != 0:\n",
        "            confidence = ((0.5-predictions[i]) * 2) if overall_predictions[i] == 0 else ((predictions[i]-0.5) * 2)\n",
        "            if confidence >= minimum_confidence:\n",
        "                amount_of_trades += 1\n",
        "                if overall_predictions[i] == results[i]:\n",
        "                    win_confidence_tracker.append(confidence)\n",
        "                    win_confidence_tracker.append(confidence)\n",
        "                    wins += 1\n",
        "                else:\n",
        "                    loss_confidence_tracker.append(confidence)\n",
        "    plt.clf()\n",
        "    plt.hist(win_confidence_tracker, bins=100, alpha=0.5, label='Correct')\n",
        "    plt.hist(loss_confidence_tracker, bins=100, alpha=0.5, label='Incorrect')\n",
        "    plt.legend(loc='upper right')\n",
        "    record_graph(pair,\"win_rate_confidence_level_with_RR_over_90_confidence\",plt)\n",
        "    try:\n",
        "        win_rate = wins / amount_of_trades\n",
        "    except:\n",
        "        win_rate = 0\n",
        "    record_stat(pair,\"win rate using over 0.9 confidence = \" + str(win_rate))\n",
        "\n",
        "def over_time_balance_graph(pair,overall_predictions,results):\n",
        "    trade_tracker = [0]\n",
        "    for i in range(len(predictions)):\n",
        "        if overall_predictions[i] != 0:\n",
        "            confidence = ((0.5-predictions[i]) * 2) if overall_predictions[i] == 0 else ((predictions[i]-0.5) * 2)\n",
        "            if confidence >= minimum_confidence:\n",
        "                if overall_predictions[i] == results[i]:\n",
        "                    trade_tracker.append(trade_tracker[-1] + 2)\n",
        "                else:\n",
        "                    trade_tracker.append(trade_tracker[-1] - 1)\n",
        "    plt.clf()\n",
        "    plt.plot(trade_tracker)\n",
        "    record_graph(pair,\"over_time_balance_graph\",plt)\n",
        "    record_stat(pair,\"amount_of_trades = \" + str(len(trade_tracker)-1))\n",
        "    record_stat(pair,\"total_oppertunities = \" + str(len(predictions)))\n",
        "\n",
        "def evaluation(pair,predictions,overall_predictions,results):\n",
        "    create_evaluation_folder(pair)\n",
        "    predicted_wins_and_losses_amount(pair,overall_predictions)\n",
        "    accuracy(pair,overall_predictions,results)\n",
        "    win_rate(pair,overall_predictions,results)\n",
        "    accuracy_confidence_level(pair,predictions,overall_predictions,results)\n",
        "    win_rate_confidence_level(pair,predictions,overall_predictions,results)\n",
        "    win_rate_confidence_level_with_RR(pair,predictions,overall_predictions,results)\n",
        "    win_rate_confidence_level_with_RR_over_90_confidence(pair,predictions,overall_predictions,results)\n",
        "    over_time_balance_graph(pair,overall_predictions,results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Make predictions on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the best model\n",
        "clf = load_model(\"model/best_model/\", custom_objects={\"win_rate\": win_rate})\n",
        "\n",
        "# remove .DS_Store from \"image_data/validation\"\n",
        "if os.path.exists(\"image_data/validation/.DS_Store\"):\n",
        "    os.remove(\"image_data/validation/.DS_Store\")\n",
        "\n",
        "# for each folder in image_data/validation\n",
        "for folder in os.listdir(\"image_data/validation\"):\n",
        "    pair = folder\n",
        "    predictions = []\n",
        "    results = []\n",
        "    files = os.listdir(\"image_data/validation/\" + folder + \"/win\") + os.listdir(\"image_data/validation/\" + folder + \"/loss\")\n",
        "    files.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "\n",
        "    # reduce the amount of files to 10\n",
        "    # files = files[:10]\n",
        "\n",
        "    for image in files:\n",
        "        win = 0\n",
        "        try:\n",
        "            image = tf.keras.preprocessing.image.load_img(\n",
        "                \"image_data/validation/\" + folder + \"/win/\" + image, target_size=(200, 50), color_mode=\"grayscale\"\n",
        "            )\n",
        "            win = 1\n",
        "        except:\n",
        "            image = tf.keras.preprocessing.image.load_img(\n",
        "                \"image_data/validation/\" + folder + \"/loss/\" + image, target_size=(200, 50), color_mode=\"grayscale\"\n",
        "            )\n",
        "\n",
        "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        prediction = clf.predict(image, verbose=0)\n",
        "        predictions.append(prediction[0][0])\n",
        "        results.append(win)\n",
        "        # print the percentage of images that have been predicted on the same line\n",
        "        print(str(round(((len(predictions) / len(files))/len(os.listdir(\"image_data/validation\"))) * 100, 2)) + \"%\", end=\"\\r\")\n",
        "    # convert predictions to np array\n",
        "    predictions = np.array(predictions)\n",
        "    # create a list overall_predictions that converts the predictions to 0 or 1\n",
        "    overall_predictions = np.where(predictions < 0.5, 0, 1)\n",
        "    # evaluate the model\n",
        "    evaluation(pair,predictions,overall_predictions,results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
